{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: Day 20\n",
    "__1. Read this url and find the 10 most frequent words. romeo_and_juliet = 'http://www.gutenberg.org/files/1112/1112.txt'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most frequent words are:\n",
      "the: 844\n",
      "and: 761\n",
      "to: 630\n",
      "i: 597\n",
      "a: 528\n",
      "of: 503\n",
      "in: 376\n",
      "my: 374\n",
      "you: 363\n",
      "is: 362\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Fetching the text from the URL\n",
    "url = 'https://www.gutenberg.org/files/1112/1112-0.txt'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    # Extracting text content\n",
    "    text = response.text\n",
    "    \n",
    "    # Removing punctuation and converting text to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    \n",
    "    # Splitting the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Counting the frequency of each word\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Getting the 10 most common words\n",
    "    most_common_words = word_freq.most_common(10)\n",
    "    \n",
    "    print(\"The 10 most frequent words are:\")\n",
    "    for word, frequency in most_common_words:\n",
    "        print(f\"{word}: {frequency}\")\n",
    "else:\n",
    "    print(\"Failed to fetch the content from the URL\")\n",
    "    print('status_code is: ', response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Read the cats API and cats_api = 'https://api.thecatapi.com/v1/breeds' and find :__\n",
    "   1. the min, max, mean, median, standard deviation of cats' weight in metric units.\n",
    "   2. the min, max, mean, median, standard deviation of cats' lifespan in years.\n",
    "   3. Create a frequency table of country and breed of cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for cat weights (in metric units):\n",
      "Min: 2.0\n",
      "Max: 5.0\n",
      "Mean: 3.2238805970149254\n",
      "Median: 3.0\n",
      "Standard Deviation: 0.8845628182703051\n",
      "\n",
      "Statistics for cat lifespans (in years):\n",
      "Min: 8.0\n",
      "Max: 18.0\n",
      "Mean: 12.074626865671641\n",
      "Median: 12.0\n",
      "Standard Deviation: 1.8283411328456127\n",
      "\n",
      "Frequency table of country and breed of cats:\n",
      "EG - Abyssinian: 1\n",
      "GR - Aegean: 1\n",
      "US - American Bobtail: 1\n",
      "US - American Curl: 1\n",
      "US - American Shorthair: 1\n",
      "US - American Wirehair: 1\n",
      "AE - Arabian Mau: 1\n",
      "AU - Australian Mist: 1\n",
      "US - Balinese: 1\n",
      "US - Bambino: 1\n",
      "US - Bengal: 1\n",
      "FR - Birman: 1\n",
      "US - Bombay: 1\n",
      "GB - British Longhair: 1\n",
      "GB - British Shorthair: 1\n",
      "MM - Burmese: 1\n",
      "GB - Burmilla: 1\n",
      "US - California Spangled: 1\n",
      "US - Chantilly-Tiffany: 1\n",
      "FR - Chartreux: 1\n",
      "EG - Chausie: 1\n",
      "US - Cheetoh: 1\n",
      "US - Colorpoint Shorthair: 1\n",
      "GB - Cornish Rex: 1\n",
      "CA - Cymric: 1\n",
      "CY - Cyprus: 1\n",
      "GB - Devon Rex: 1\n",
      "RU - Donskoy: 1\n",
      "CN - Dragon Li: 1\n",
      "EG - Egyptian Mau: 1\n",
      "MM - European Burmese: 1\n",
      "US - Exotic Shorthair: 1\n",
      "GB - Havana Brown: 1\n",
      "US - Himalayan: 1\n",
      "JP - Japanese Bobtail: 1\n",
      "US - Javanese: 1\n",
      "TH - Khao Manee: 1\n",
      "TH - Korat: 1\n",
      "RU - Kurilian: 1\n",
      "TH - LaPerm: 1\n",
      "US - Maine Coon: 1\n",
      "GB - Malayan: 1\n",
      "IM - Manx: 1\n",
      "US - Munchkin: 1\n",
      "US - Nebelung: 1\n",
      "NO - Norwegian Forest Cat: 1\n",
      "US - Ocicat: 1\n",
      "US - Oriental: 1\n",
      "IR - Persian: 1\n",
      "US - Pixie-bob: 1\n",
      "US - Ragamuffin: 1\n",
      "US - Ragdoll: 1\n",
      "RU - Russian Blue: 1\n",
      "US - Savannah: 1\n",
      "GB - Scottish Fold: 1\n",
      "US - Selkirk Rex: 1\n",
      "TH - Siamese: 1\n",
      "RU - Siberian: 1\n",
      "SP - Singapura: 1\n",
      "US - Snowshoe: 1\n",
      "SO - Somali: 1\n",
      "CA - Sphynx: 1\n",
      "CA - Tonkinese: 1\n",
      "US - Toyger: 1\n",
      "TR - Turkish Angora: 1\n",
      "TR - Turkish Van: 1\n",
      "US - York Chocolate: 1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "\n",
    "# Fetching data from the API\n",
    "response = requests.get(cats_api)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    cat_data = response.json()\n",
    "    \n",
    "    # Lists to store weights and lifespans\n",
    "    weights = []\n",
    "    lifespans = []\n",
    "    \n",
    "    # Dictionary for frequency table of country and breed\n",
    "    country_breed_freq = Counter()\n",
    "    \n",
    "    for cat in cat_data:\n",
    "        # Handling weight data\n",
    "        if 'weight' in cat and 'metric' in cat['weight']:\n",
    "            weight_str = cat['weight']['metric']\n",
    "            weight = float(weight_str.split()[0]) if weight_str else None\n",
    "            if weight:\n",
    "                weights.append(weight)\n",
    "        \n",
    "        # Handling lifespan data\n",
    "        if 'life_span' in cat:\n",
    "            lifespan_str = cat['life_span']\n",
    "            lifespan = float(lifespan_str.split()[0]) if lifespan_str else None\n",
    "            if lifespan:\n",
    "                lifespans.append(lifespan)\n",
    "        \n",
    "        # Some entries might not have 'country_code' or 'name' field, so check if they exist\n",
    "        if 'country_code' in cat and 'name' in cat:\n",
    "            country_code = cat['country_code']\n",
    "            breed_name = cat['name']\n",
    "            \n",
    "            # Incrementing the count for the (country, breed) pair\n",
    "            country_breed_freq[(country_code, breed_name)] += 1\n",
    "    \n",
    "    # Calculations for weights\n",
    "    min_weight = min(weights) if weights else None\n",
    "    max_weight = max(weights) if weights else None\n",
    "    mean_weight = statistics.mean(weights) if weights else None\n",
    "    median_weight = statistics.median(weights) if weights else None\n",
    "    stdev_weight = statistics.stdev(weights) if len(weights) > 1 else None\n",
    "    \n",
    "    # Calculations for lifespans\n",
    "    min_lifespan = min(lifespans) if lifespans else None\n",
    "    max_lifespan = max(lifespans) if lifespans else None\n",
    "    mean_lifespan = statistics.mean(lifespans) if lifespans else None\n",
    "    median_lifespan = statistics.median(lifespans) if lifespans else None\n",
    "    stdev_lifespan = statistics.stdev(lifespans) if len(lifespans) > 1 else None\n",
    "    \n",
    "    # Displaying results\n",
    "    print(\"Statistics for cat weights (in metric units):\")\n",
    "    print(f\"Min: {min_weight}\")\n",
    "    print(f\"Max: {max_weight}\")\n",
    "    print(f\"Mean: {mean_weight}\")\n",
    "    print(f\"Median: {median_weight}\")\n",
    "    print(f\"Standard Deviation: {stdev_weight}\")\n",
    "    \n",
    "    print(\"\\nStatistics for cat lifespans (in years):\")\n",
    "    print(f\"Min: {min_lifespan}\")\n",
    "    print(f\"Max: {max_lifespan}\")\n",
    "    print(f\"Mean: {mean_lifespan}\")\n",
    "    print(f\"Median: {median_lifespan}\")\n",
    "    print(f\"Standard Deviation: {stdev_lifespan}\")\n",
    "    \n",
    "    print(\"\\nFrequency table of country and breed of cats:\")\n",
    "    for (country, breed), count in country_breed_freq.items():\n",
    "        print(f\"{country} - {breed}: {count}\")\n",
    "else:\n",
    "    print(\"Failed to fetch data from the API\")\n",
    "    print('status_code is: ', response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Read the [countries API](https://restcountries.eu/rest/v2/all) and find**\n",
    "   1. the 10 largest countries\n",
    "   2. the 10 most spoken languages\n",
    "   3. the total number of languages in the countries API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 largest countries by area:\n",
      "Russian Federation: 17124442.0 square kilometers\n",
      "Antarctica: 14000000.0 square kilometers\n",
      "Canada: 9984670.0 square kilometers\n",
      "China: 9640011.0 square kilometers\n",
      "United States of America: 9629091.0 square kilometers\n",
      "Brazil: 8515767.0 square kilometers\n",
      "Australia: 7692024.0 square kilometers\n",
      "India: 3287590.0 square kilometers\n",
      "Argentina: 2780400.0 square kilometers\n",
      "Kazakhstan: 2724900.0 square kilometers\n",
      "\n",
      "The 10 most spoken languages:\n",
      "English: 91 countries\n",
      "French: 44 countries\n",
      "Arabic: 25 countries\n",
      "Spanish: 24 countries\n",
      "Portuguese: 10 countries\n",
      "Russian: 8 countries\n",
      "Dutch: 8 countries\n",
      "German: 7 countries\n",
      "Chinese: 5 countries\n",
      "Serbian: 4 countries\n",
      "\n",
      "The total number of languages in the countries API: 123\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# API endpoint for countries\n",
    "countries_api = 'https://restcountries.com/v2/all'\n",
    "\n",
    "# Fetching data from the API\n",
    "response = requests.get(countries_api)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    countries_data = response.json()\n",
    "    \n",
    "    # Finding the 10 largest countries by area\n",
    "    largest_countries = sorted(countries_data, key=lambda x: x.get('area', 0), reverse=True)[:10]\n",
    "    \n",
    "    # Finding the 10 most spoken languages\n",
    "    all_languages = []\n",
    "    for country in countries_data:\n",
    "        all_languages.extend(country.get('languages', []))\n",
    "    spoken_languages_count = {language['name']: all_languages.count(language) for language in all_languages}\n",
    "    top_spoken_languages = sorted(spoken_languages_count.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    # Finding the total number of unique languages in the API\n",
    "    unique_languages = set()\n",
    "    for country in countries_data:\n",
    "        unique_languages.update([lang['name'] for lang in country.get('languages', [])])\n",
    "    total_languages = len(unique_languages)\n",
    "    \n",
    "    # Displaying the results\n",
    "    print(\"The 10 largest countries by area:\")\n",
    "    for country in largest_countries:\n",
    "        print(f\"{country['name']}: {country.get('area')} square kilometers\")\n",
    "    \n",
    "    print(\"\\nThe 10 most spoken languages:\")\n",
    "    for language, count in top_spoken_languages:\n",
    "        print(f\"{language}: {count} countries\")\n",
    "    \n",
    "    print(f\"\\nThe total number of languages in the countries API: {total_languages}\")\n",
    "else:\n",
    "    print(\"Failed to fetch data from the Countries API\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. UCI is one of the most common places to get data sets for data science and machine learning. Read the content of UCL (https://archive.ics.uci.edu/ml/datasets.php). Without additional libraries it will be difficult, so you may try it with BeautifulSoup4__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "UCI Machine Learning Repository\n",
      "\n",
      "Datasets - UCI Machine Learning Repository\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "       Datasets Contribute Dataset Donate New Link External About Us Who We Are Citation Metadata Contact Information           Login    Filters            Keywords     Data Type      Subject Area      Task      # Features      # Instances      Feature Type      Python    Browse Datasets   Filters  Sort by # Views, desc # Views   Name  # Instances  # Features  Date Donated  Relevance        Expand All Collapse All    Internet Advertisements This dataset represents a set of possible advertisements on Internet pages.  Classification  Multivariate  3.28K Instances  1.56K Features      Heart failure clinical records This dataset contains the medical records of 299 patients who had heart failure, collected during their follow-up period, where each patient profile has 13 clinical features.  Classification, Regression, Clustering  Multivariate  299 Instances  12 Features      Iranian Churn Dataset This dataset is randomly collected from an Iranian telecom companyâ€™s database over a period of 12 months.  Classification, Regression  Multivariate  3.15K Instances  13 Features      Syskill and Webert Web Page Ratings This database contains HTML source of web pages plus the ratings of a single user on these web pages. Web pages are on four seperate subjects (Bands- recording artists; Goats; Sheep; and BioMedical)  Classification  Multivariate, Text  332 Instances  5 Features      Cirrhosis Patient Survival Prediction Utilize 17 clinical features for predicting survival state of patients with liver cirrhosis. The survival states include 0 = D (death), 1 = C (censored), 2 = CL (censored due to liver transplantation).  Classification  Tabular  418 Instances  17 Features      Plants Data has been extracted from the USDA plants database. It contains all plants (species and genera) in the database and the states of USA and Canada where they occur.  Clustering  Multivariate  34.78K Instances  70 Features      Toxicity The dataset includes 171 molecules designed for functional domains of a core clock protein, CRY1, responsible for generating circadian rhythm. 56 of the molecules are toxic and the rest are non-toxic.  Classification  Tabular  171 Instances  1.2K Features      Bone marrow transplant: children The data set describes pediatric patients with several hematologic diseases, who were subject to the unmanipulated allogeneic unrelated donor hematopoietic stem cell transplantation.  Classification, Regression  Multivariate  187 Instances  36 Features      accelerometer_gyro_mobile_phone_dataset data collected on 2022, in King Saud University in riyadh for recognizing human activities using mobile phone IMU sensors (Accelerometer, and Gyroscope). these activity is calssified to standing(stop), or walking.  Classification  Tabular, Sequential, Multivariate, Time-Series  31.99K Instances  8 Features      SML2010 This dataset is collected from a monitor system mounted in a domotic house. It corresponds to approximately 40 days of monitoring data.  Regression  Multivariate, Sequential, Time-Series, Text  4.14K Instances  24 Features    Rows per page 510152025 0 to 10 of 12      Filters            Keywords     Data Type      Subject Area      Task      # Features      # Instances      Feature Type      Python     By using the UCI Machine Learning Repository,\n",
      "you acknowledge and accept the cookies and privacy practices used by the UCI Machine Learning Repository. Accept Read Policy  The Project About Us CML National Science Foundation Navigation Home View Datasets Donate a Dataset Logistics Contact Privacy Notice Feature Request or Bug Report  Browse Datasets Donate a Dataset Link an external Dataset  Who We Are Citation Metadata Contact Information   Login \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of UCI Machine Learning Repository\n",
    "uci_url = 'https://archive.ics.uci.edu/datasets?skip=0&take=10&sort=desc&orderBy=NumHits&search=ml'\n",
    "\n",
    "# Fetching the content of the page\n",
    "response = requests.get(uci_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parsing the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Finding and displaying the text content\n",
    "    page_content = soup.get_text()\n",
    "    print(page_content)\n",
    "else:\n",
    "    print(\"Failed to fetch the content from the UCI website\")\n",
    "    print(\"Status code is:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
